\contentsline {chapter}{\numberline {1}Computer Architecture}{1}{chapter.1}%
\contentsline {section}{\numberline {1.1}How is Data Represented?}{1}{section.1.1}%
\contentsline {subsection}{\numberline {1.1.1}Integer Data Representation}{1}{subsection.1.1.1}%
\contentsline {subsection}{\numberline {1.1.2}Variables and Data Allocation of Different Lifetimes}{7}{subsection.1.1.2}%
\contentsline {subsection}{\numberline {1.1.3}Program and Data}{9}{subsection.1.1.3}%
\contentsline {section}{\numberline {1.2}Basic Computer Organization}{11}{section.1.2}%
\contentsline {subsection}{\numberline {1.2.1}Main Memory and its problems}{13}{subsection.1.2.1}%
\contentsline {subsection}{\numberline {1.2.2}Cache Memory and Locality of Reference}{14}{subsection.1.2.2}%
\contentsline {subsection}{\numberline {1.2.3}Cache Composition}{15}{subsection.1.2.3}%
\contentsline {paragraph}{Advantages of Direct-Mapped Cache:}{18}{Item.23}%
\contentsline {paragraph}{Disadvantages of Direct-Mapped Cache:}{18}{Item.23}%
\contentsline {section}{\numberline {1.3}Cache and Programming}{23}{section.1.3}%
\contentsline {subsection}{\numberline {1.3.1}Example 1: Vector Sum Reduction}{23}{subsection.1.3.1}%
\contentsline {subsection}{\numberline {1.3.2}Example 2: Vector Dot Product}{25}{subsection.1.3.2}%
\contentsline {subsection}{\numberline {1.3.3}Example 3: DAXPY}{28}{subsection.1.3.3}%
\contentsline {subsection}{\numberline {1.3.4}Example 4: 2D Matrix Sum}{28}{subsection.1.3.4}%
\contentsline {subsection}{\numberline {1.3.5}Example 5: Matrix Multiplication}{31}{subsection.1.3.5}%
\contentsline {section}{\numberline {1.4}Vector Operations}{37}{section.1.4}%
\contentsline {subsection}{\numberline {1.4.1}Strip mining}{37}{subsection.1.4.1}%
\contentsline {subsection}{\numberline {1.4.2}Node Splitting}{38}{subsection.1.4.2}%
\contentsline {subsection}{\numberline {1.4.3}Scalar Expansion}{39}{subsection.1.4.3}%
\contentsline {subsection}{\numberline {1.4.4}Loop fission}{39}{subsection.1.4.4}%
\contentsline {subsection}{\numberline {1.4.5}Loop interchange}{40}{subsection.1.4.5}%
\contentsline {subsection}{\numberline {1.4.6}Summary of Techniques}{40}{subsection.1.4.6}%
\contentsline {chapter}{\numberline {2}Parallel Architecture}{41}{chapter.2}%
\contentsline {section}{\numberline {2.1}Introduction}{41}{section.2.1}%
\contentsline {section}{\numberline {2.2}Classification of Architectures - Flynn's Taxonomy}{42}{section.2.2}%
\contentsline {subsection}{\numberline {2.2.1}SIMD - Single Instruction Multiple Data}{42}{subsection.2.2.1}%
\contentsline {subsection}{\numberline {2.2.2}MISD - Multiple Instruction Single Data}{43}{subsection.2.2.2}%
\contentsline {subsection}{\numberline {2.2.3}MIMD - Mutliple Instruction Multiple Data}{44}{subsection.2.2.3}%
\contentsline {section}{\numberline {2.3}Classification based on Memory}{44}{section.2.3}%
\contentsline {subsection}{\numberline {2.3.1}Shared Memory}{44}{subsection.2.3.1}%
\contentsline {subsubsection}{Uniform Memory Access (UMA)}{45}{figure.caption.17}%
\contentsline {subsubsection}{Non-Uniform Memory Access (NUMA)}{46}{figure.caption.18}%
\contentsline {subsection}{\numberline {2.3.2}Distributed Memory Architecture}{47}{subsection.2.3.2}%
\contentsline {subsection}{\numberline {2.3.3}Shared Memory Architectures: Cache Coherence}{47}{subsection.2.3.3}%
\contentsline {subsubsection}{State Transition Diagram}{48}{subsection.2.3.3}%
\contentsline {subsection}{\numberline {2.3.4}Implementation of Cache Coherence Protocols}{49}{subsection.2.3.4}%
\contentsline {subsubsection}{Snooping Protocol}{49}{subsection.2.3.4}%
\contentsline {subsubsection}{Directory Based Protocol}{49}{subsection.2.3.4}%
\contentsline {subsection}{\numberline {2.3.5}False Sharing}{50}{subsection.2.3.5}%
\contentsline {section}{\numberline {2.4}Interconnection networks for a Parallel Computer}{51}{section.2.4}%
\contentsline {subsection}{\numberline {2.4.1}Network Topology}{51}{subsection.2.4.1}%
\contentsline {subsubsection}{Bus-based networks}{51}{subsection.2.4.1}%
\contentsline {subsubsection}{Cross-bar Networks}{52}{figure.caption.23}%
\contentsline {subsubsection}{Multistage Network}{53}{figure.caption.24}%
\contentsline {subsubsection}{Completely Connected Networks}{56}{figure.caption.28}%
\contentsline {subsubsection}{Star Connected Network/Star Topology}{57}{figure.caption.29}%
\contentsline {subsubsection}{Linear Arrays, Meshes and k-d Meshes}{57}{figure.caption.30}%
\contentsline {subsubsection}{Tree Based Networks}{59}{figure.caption.34}%
\contentsline {subsection}{\numberline {2.4.2}Evaluating Static Networks}{60}{subsection.2.4.2}%
\contentsline {subsubsection}{Diameter}{61}{subsection.2.4.2}%
\contentsline {subsubsection}{Connectivity}{61}{subsection.2.4.2}%
\contentsline {subsubsection}{Bisection Width and Bisection Bandwidth}{61}{subsection.2.4.2}%
\contentsline {subsubsection}{Cost}{62}{equation.2.4.4}%
\contentsline {section}{\numberline {2.5}Graphical Processing Units}{63}{section.2.5}%
\contentsline {subsection}{\numberline {2.5.1}GPU Architecture}{63}{subsection.2.5.1}%
\contentsline {subsection}{\numberline {2.5.2}CUDA Memory Spaces}{65}{subsection.2.5.2}%
\contentsline {section}{\numberline {2.6}Parallelization Principles}{68}{section.2.6}%
\contentsline {subsection}{\numberline {2.6.1}Evaluation of Parallel Programs}{68}{subsection.2.6.1}%
\contentsline {section}{\numberline {2.7}Parallel Programming Classification and Steps}{72}{section.2.7}%
\contentsline {subsection}{\numberline {2.7.1}Parallel Program Models}{72}{subsection.2.7.1}%
\contentsline {subsection}{\numberline {2.7.2}Programming Paradigms}{72}{subsection.2.7.2}%
\contentsline {subsection}{\numberline {2.7.3}Parallelizing a Program}{72}{subsection.2.7.3}%
\contentsline {subsection}{\numberline {2.7.4}Data Parallelism and Data Decomposition}{73}{subsection.2.7.4}%
\contentsline {subsection}{\numberline {2.7.5}Data Distributions}{73}{subsection.2.7.5}%
\contentsline {subsection}{\numberline {2.7.6}Task Parallelism}{74}{subsection.2.7.6}%
\contentsline {subsection}{\numberline {2.7.7}Orchestration}{75}{subsection.2.7.7}%
\contentsline {subsubsection}{Maximizing data locality: }{75}{subsection.2.7.7}%
\contentsline {subsubsection}{Minimizing contention and hotspots: }{76}{subsection.2.7.7}%
\contentsline {subsubsection}{Overlapping computations with interactions}{76}{table.caption.45}%
\contentsline {subsubsection}{Replicating data or computations }{77}{table.caption.45}%
\contentsline {subsection}{\numberline {2.7.8}Mapping}{77}{subsection.2.7.8}%
\contentsline {subsection}{\numberline {2.7.9}Example}{78}{subsection.2.7.9}%
\contentsline {subsection}{\numberline {2.7.10}Notes on Message Passing Version}{88}{subsection.2.7.10}%
\contentsline {subsection}{\numberline {2.7.11}Send and Receive Alternatives}{89}{subsection.2.7.11}%
\contentsline {subsection}{\numberline {2.7.12}Summary}{90}{subsection.2.7.12}%
\contentsline {section}{\numberline {2.8}Shared Memory Parallelism - OpenMP}{90}{section.2.8}%
\contentsline {subsection}{\numberline {2.8.1}Introduction}{90}{subsection.2.8.1}%
\contentsline {subsection}{\numberline {2.8.2}Fork-Join Model}{91}{subsection.2.8.2}%
\contentsline {subsection}{\numberline {2.8.3}Parallel Construct}{92}{subsection.2.8.3}%
\contentsline {subsection}{\numberline {2.8.4}Work sharing construct}{94}{subsection.2.8.4}%
\contentsline {subsubsection}{for-loop}{94}{subsection.2.8.4}%
\contentsline {subsubsection}{Coarse Level parallelism - Sections and Tasks}{96}{lstnumber.2.7.19}%
\contentsline {subsection}{\numberline {2.8.5}Synchronization Directives}{97}{subsection.2.8.5}%
\contentsline {subsection}{\numberline {2.8.6}Data Scope Attribute Clauses}{99}{subsection.2.8.6}%
\contentsline {subsubsection}{threadprivate}{100}{Item.46}%
\contentsline {subsubsection}{default-example}{101}{lstnumber.2.11.13}%
\contentsline {subsection}{\numberline {2.8.7}Library Routines (API)}{101}{subsection.2.8.7}%
\contentsline {subsection}{\numberline {2.8.8}Locks}{103}{subsection.2.8.8}%
\contentsline {subsection}{\numberline {2.8.9}Example 1: Jacobi Solver}{104}{subsection.2.8.9}%
\contentsline {subsection}{\numberline {2.8.10}Breadth First Search (BFS) Algorithm}{105}{subsection.2.8.10}%
\contentsline {subsubsection}{Version 1 (Nested Parallelism)}{105}{subsection.2.8.10}%
\contentsline {subsubsection}{Version 2 (Task constructs)}{106}{lstnumber.2.16.22}%
\contentsline {section}{\numberline {2.9}Message Passing Interface - MPI}{108}{section.2.9}%
\contentsline {subsection}{\numberline {2.9.1}MPI Intrdocution}{108}{subsection.2.9.1}%
\contentsline {subsection}{\numberline {2.9.2}Communication Primitives}{109}{subsection.2.9.2}%
\contentsline {subsubsection}{Point-Point Communications}{109}{subsection.2.9.2}%
\contentsline {subsubsection}{EXAMPLE 1: Finding maximum using 2 processes}{111}{lstnumber.-33.12}%
\contentsline {subsection}{\numberline {2.9.3}Buffering and Safety}{113}{subsection.2.9.3}%
\contentsline {subsubsection}{Blocking Communications}{113}{subsection.2.9.3}%
\contentsline {subsubsection}{Non-Blocking Communications}{114}{subsection.2.9.3}%
\contentsline {subsection}{\numberline {2.9.4}Example: Finding a Particular element in an Array}{116}{subsection.2.9.4}%
\contentsline {subsection}{\numberline {2.9.5}Communication Modes}{117}{subsection.2.9.5}%
